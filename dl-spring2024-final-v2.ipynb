{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2542390,"sourceType":"datasetVersion","datasetId":1541666},{"sourceId":8377668,"sourceType":"datasetVersion","datasetId":4981571}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nimport re\nimport h5py\nimport math\nimport shutil\nimport random\nimport nibabel\nimport tarfile\nimport nibabel as nib\nimport SimpleITK as sitk\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom tqdm import tqdm\nfrom nilearn import plotting\nfrom nilearn.plotting import plot_anat, plot_roi\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom sklearn.model_selection import train_test_split\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(filename, os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-12T03:50:41.294028Z","iopub.execute_input":"2024-05-12T03:50:41.294947Z","iopub.status.idle":"2024-05-12T03:50:41.303890Z","shell.execute_reply.started":"2024-05-12T03:50:41.294872Z","shell.execute_reply":"2024-05-12T03:50:41.302719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install einops\n!pip install medpy\nfrom einops import rearrange\nfrom medpy import metric","metadata":{"execution":{"iopub.status.busy":"2024-05-12T03:50:43.906156Z","iopub.execute_input":"2024-05-12T03:50:43.907156Z","iopub.status.idle":"2024-05-12T03:51:26.306810Z","shell.execute_reply.started":"2024-05-12T03:50:43.907121Z","shell.execute_reply":"2024-05-12T03:51:26.305433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preset","metadata":{}},{"cell_type":"code","source":"!mkdir checkpoint\n!mkdir results\n!mkdir data\n!mkdir dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-11T20:04:06.619514Z","iopub.execute_input":"2024-05-11T20:04:06.619936Z","iopub.status.idle":"2024-05-11T20:04:10.545046Z","shell.execute_reply.started":"2024-05-11T20:04:06.619901Z","shell.execute_reply":"2024-05-11T20:04:10.543924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# shutil.rmtree('./dataset')","metadata":{"execution":{"iopub.status.busy":"2024-05-11T20:04:10.546519Z","iopub.execute_input":"2024-05-11T20:04:10.546889Z","iopub.status.idle":"2024-05-11T20:04:10.551830Z","shell.execute_reply.started":"2024-05-11T20:04:10.546858Z","shell.execute_reply":"2024-05-11T20:04:10.550844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set = {\n        'root': './data',    # input path to original dataset of 4 labels\n        'out': './dataset',  # output path to preprocessed dataset\n        'flist': '/kaggle/input/train-list/train.txt',  # training IDs\n        }\n\n# if origianal dataset folder is empty, extract the dataset from input\nif len(os.listdir(train_set['root'])) == 0 and len(os.listdir(train_set['out'])) == 0:\n    zip_file = tarfile.open(\"/kaggle/input/brats-2021-task1/BraTS2021_Training_Data.tar\")\n    zip_file.extractall(train_set['root'])\n    zip_file.close()","metadata":{"execution":{"iopub.status.busy":"2024-05-12T03:46:13.686442Z","iopub.execute_input":"2024-05-12T03:46:13.686876Z","iopub.status.idle":"2024-05-12T03:46:13.694898Z","shell.execute_reply.started":"2024-05-12T03:46:13.686843Z","shell.execute_reply":"2024-05-12T03:46:13.693576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Preprocessing","metadata":{}},{"cell_type":"code","source":"# mri images of four modalities\nmodalities = ('flair', 't1ce', 't1', 't2')\n\ndef process_h5(path, out_path):\n    \"\"\" Save the data with dtype=float32.\n        z-score is used but keep the background with zero! \"\"\"\n    if not os.path.exists(path + 'seg.nii.gz'): return\n\n    # SimpleITK reads images in DxHxD by defaultï¼Œconvert it to HxWxD here\n    label = sitk.GetArrayFromImage(sitk.ReadImage(path + 'seg.nii.gz')).transpose(1,2,0)\n    print(label.shape)\n    # stack images of four modalities 4 x (H,W,D) -> (4,H,W,D) \n    images = np.stack([sitk.GetArrayFromImage(sitk.ReadImage(path + modal + '.nii.gz')).transpose(1,2,0) for modal in modalities], 0)  # [240,240,155]\n    # datatype converting\n    label = label.astype(np.uint8)\n    images = images.astype(np.float32)\n    case_name = path.split('/')[-1]\n    # case_name = os.path.split(path)[-1]  # different paths for windows and linux\n    \n    path = os.path.join(out_path,case_name)\n    path_to_rm = os.path.join('./data',case_name)[:-1]\n    output = path + 'mri_norm2.h5'\n    # print('path_to_rm', path_to_rm)\n    if os.path.exists(output):\n        shutil.rmtree(path_to_rm)\n        return\n    \n    # sum up the first channel, if all four modalities are 0, mark it as background (False):\n    mask = images.sum(0) > 0\n    for k in range(4):\n\n        x = images[k,...]  #\n        y = x[mask]\n\n        # normalize the region outside the background\n        x[mask] -= y.mean()\n        x[mask] /= y.std()\n\n        images[k,...] = x\n    print(case_name, images.shape, label.shape)\n    f = h5py.File(output, 'w')\n    f.create_dataset('image', data=images, compression=\"gzip\")\n    f.create_dataset('label', data=label, compression=\"gzip\")\n    f.close()\n    \n    # remove the original dataset to save space\n    shutil.rmtree(path_to_rm)\n\n\ndef pre_data(dset):\n    root, out_path = dset['root'], dset['out']\n    file_list = os.path.join(root, dset['flist'])\n    subjects = open(file_list).read().splitlines()\n    names = ['BraTS2021_' + sub for sub in subjects]\n    names = random.sample(names, 125)\n    paths = [os.path.join(root, name, name + '_') for name in names]\n\n    for path in tqdm(paths):\n        process_h5(path, out_path)\n\n    print('Finished')\n\npre_data(train_set)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T03:48:08.157249Z","iopub.execute_input":"2024-05-12T03:48:08.158065Z","iopub.status.idle":"2024-05-12T03:48:08.188792Z","shell.execute_reply.started":"2024-05-12T03:48:08.158027Z","shell.execute_reply":"2024-05-12T03:48:08.187625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    print(\"CUDA is available. GPU will be used for training.\")\n    device = torch.device(\"cuda\")\nelse:\n    print(\"CUDA is not available. Training will be on CPU.\")\n    device = torch.device(\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-05-12T03:48:46.824165Z","iopub.execute_input":"2024-05-12T03:48:46.824592Z","iopub.status.idle":"2024-05-12T03:48:46.908818Z","shell.execute_reply.started":"2024-05-12T03:48:46.824557Z","shell.execute_reply":"2024-05-12T03:48:46.907671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_and_test_set_ids = os.listdir(train_set['out'])\ntrain_and_test_ids = [i[:15] for i in train_and_test_set_ids]\n\n# randomly select 125 groups of sata from 1251 groups\ntrain_and_test_ids = random.sample(train_and_test_ids, 125)\n\ntrain_ids, val_test_ids = train_test_split(train_and_test_ids, test_size=0.2,random_state=21)\nval_ids, test_ids = train_test_split(val_test_ids, test_size=0.5,random_state=21)\nprint(\"Using {} images for training, {} images for validation, {} images for testing.\".format(len(train_ids),len(val_ids),len(test_ids)))\n\ntrain_ids.sort()\nval_ids.sort()\ntest_ids.sort()\n\nwith open('./train.txt','w') as f:\n    f.write('\\n'.join(train_ids))\n\nwith open('./valid.txt','w') as f:\n    f.write('\\n'.join(val_ids))\n\nwith open('./test.txt','w') as f:\n    f.write('\\n'.join(test_ids))","metadata":{"execution":{"iopub.status.busy":"2024-05-11T20:55:13.389962Z","iopub.execute_input":"2024-05-11T20:55:13.390257Z","iopub.status.idle":"2024-05-11T20:55:13.405424Z","shell.execute_reply.started":"2024-05-11T20:55:13.390232Z","shell.execute_reply":"2024-05-11T20:55:13.404600Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Models","metadata":{}},{"cell_type":"code","source":"# U-Net\nclass InConv(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(InConv, self).__init__()\n        self.conv = DoubleConv(in_ch, out_ch)\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\nclass Down(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(Down, self).__init__()\n        self.mpconv = nn.Sequential(\n            nn.MaxPool3d(2, 2),\n            DoubleConv(in_ch, out_ch)\n        )\n\n    def forward(self, x):\n        x = self.mpconv(x)\n        return x\n\nclass OutConv(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(OutConv, self).__init__()\n        self.conv = nn.Conv3d(in_ch, out_ch, 1)\n        # self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.conv(x)\n        # x = self.sigmoid(x)\n        return x\n\nclass DoubleConv(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(DoubleConv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv3d(in_ch, out_ch, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm3d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.Conv3d(out_ch, out_ch, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm3d(out_ch),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\nclass Up(nn.Module):\n    def __init__(self, in_ch, skip_ch,out_ch):\n        super(Up, self).__init__()\n        self.up = nn.ConvTranspose3d(in_ch, in_ch, kernel_size=2, stride=2)\n        self.conv = DoubleConv(in_ch+skip_ch, out_ch)\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        x = torch.cat([x2, x1], dim=1)\n        x = self.conv(x)\n        return x\n\n\nclass UNet(nn.Module):\n    def __init__(self, in_channels, num_classes):\n        super(UNet, self).__init__()\n        features = [32,64,128,256]\n\n        self.inc = InConv(in_channels, features[0])\n        self.down1 = Down(features[0], features[1])\n        self.down2 = Down(features[1], features[2])\n        self.down3 = Down(features[2], features[3])\n        self.down4 = Down(features[3], features[3])\n\n        self.up1 = Up(features[3], features[3], features[2])\n        self.up2 = Up(features[2], features[2], features[1])\n        self.up3 = Up(features[1], features[1], features[0])\n        self.up4 = Up(features[0], features[0], features[0])\n        self.outc = OutConv(features[0], num_classes)\n\n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        x = self.outc(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-05-12T03:48:15.225880Z","iopub.execute_input":"2024-05-12T03:48:15.226990Z","iopub.status.idle":"2024-05-12T03:48:15.251358Z","shell.execute_reply.started":"2024-05-12T03:48:15.226948Z","shell.execute_reply":"2024-05-12T03:48:15.250150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Attention U-Net\nclass AttentionBlock(nn.Module):\n    def __init__(self, in_channels_x, in_channels_g, int_channels):\n        super(AttentionBlock, self).__init__()\n        self.Wx = nn.Sequential(nn.Conv3d(in_channels_x, int_channels, kernel_size=1),\n                                nn.BatchNorm3d(int_channels))\n        self.Wg = nn.Sequential(nn.Conv3d(in_channels_g, int_channels, kernel_size=1),\n                                nn.BatchNorm3d(int_channels))\n        self.psi = nn.Sequential(nn.Conv3d(int_channels, 1, kernel_size=1),\n                                 nn.BatchNorm3d(1),\n                                 nn.Sigmoid())\n\n    def forward(self, x, g):\n        # apply the Wx to the skip connection\n        x1 = self.Wx(x)\n        g1 = self.Wg(g)\n        out = self.psi(nn.ReLU(inplace=True)(x1 + g1))\n        return out * x\n\n\nclass AttentionUpBlock(nn.Module):\n    def __init__(self, in_channels_x, in_channels_g, out_channels):\n        super(AttentionUpBlock, self).__init__()\n        # self.upsample = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=2, stride=2)\n        self.attention = AttentionBlock(in_channels_x, in_channels_g, in_channels_g)\n        self.conv_bn1 = DoubleConv(in_channels_g * 2, out_channels)\n        self.conv_bn2 = DoubleConv(out_channels, out_channels)\n\n    def forward(self, x, x_skip):\n        # note : x_skip is the skip connection and x is the input from the previous block\n        # apply the attention block to the skip connection, using x as context\n\n        x = nn.functional.interpolate(x, x_skip.shape[2:], mode='trilinear', align_corners=False)\n        x_attention = self.attention(x_skip, x)\n\n        # stack their channels to feed to both convolution blocks\n        x = torch.cat((x_attention, x), dim=1)\n        x = self.conv_bn1(x)\n        return self.conv_bn2(x)\n\n\nclass AttentionUNet(nn.Module):\n    def __init__(self, in_channels, num_classes, feature_scale=4):\n        super(AttentionUNet, self).__init__()\n        feature = [96, 192, 384, 768, 1280]\n        feature = [int(x / feature_scale) for x in feature]\n\n        self.inc = InConv(in_channels, feature[0])\n        self.down1 = Down(feature[0], feature[1])  # 48\n        self.down2 = Down(feature[1], feature[2])  # 24\n        self.down3 = Down(feature[2], feature[3])  # 12\n        self.down4 = Down(feature[3], feature[3])  # 6\n\n        self.up1 = AttentionUpBlock(feature[3], feature[3], feature[2])\n        self.up2 = AttentionUpBlock(feature[2], feature[2], feature[1])\n        self.up3 = AttentionUpBlock(feature[1], feature[1], feature[0])\n        self.up4 = AttentionUpBlock(feature[0], feature[0], feature[0])\n        self.outc = OutConv(feature[0], num_classes)\n\n    def forward(self, x):\n        # with torchsnooper.snoop():\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        x = self.outc(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-05-12T03:48:15.933720Z","iopub.execute_input":"2024-05-12T03:48:15.934105Z","iopub.status.idle":"2024-05-12T03:48:15.955081Z","shell.execute_reply.started":"2024-05-12T03:48:15.934076Z","shell.execute_reply":"2024-05-12T03:48:15.953824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Enhancing","metadata":{}},{"cell_type":"code","source":"class RandomCrop(object):\n    \"\"\"\n    Crop randomly the image in a sample\n    Args:\n    output_size (int): Desired output size\n    \"\"\"\n\n    def __init__(self, output_size):\n        self.output_size = output_size\n\n    def __call__(self, sample):\n        image, label = sample['image'], sample['label']\n\n        (c, w, h, d) = image.shape\n        w1 = np.random.randint(0, w - self.output_size[0])\n        h1 = np.random.randint(0, h - self.output_size[1])\n        d1 = np.random.randint(0, d - self.output_size[2])\n\n        label = label[w1:w1 + self.output_size[0], h1:h1 + self.output_size[1], d1:d1 + self.output_size[2]]\n        image = image[:,w1:w1 + self.output_size[0], h1:h1 + self.output_size[1], d1:d1 + self.output_size[2]]\n        return {'image': image, 'label': label}\n\n\nclass CenterCrop(object):\n    def __init__(self, output_size):\n        self.output_size = output_size\n\n    def __call__(self, sample):\n        image, label = sample['image'], sample['label']\n\n        (c,w, h, d) = image.shape\n\n        w1 = int(round((w - self.output_size[0]) / 2.))\n        h1 = int(round((h - self.output_size[1]) / 2.))\n        d1 = int(round((d - self.output_size[2]) / 2.))\n\n        label = label[w1:w1 + self.output_size[0], h1:h1 + self.output_size[1], d1:d1 + self.output_size[2]]\n        image = image[:,w1:w1 + self.output_size[0], h1:h1 + self.output_size[1], d1:d1 + self.output_size[2]]\n\n        return {'image': image, 'label': label}\n\n\nclass RandomRotFlip(object):\n    \"\"\"\n    Crop randomly flip the dataset in a sample\n    Args:\n    output_size (int): Desired output size\n    \"\"\"\n\n    def __call__(self, sample):\n        image, label = sample['image'], sample['label']\n        k = np.random.randint(0, 4)\n        image = np.stack([np.rot90(x,k) for x in image],axis=0)\n        label = np.rot90(label, k)\n        axis = np.random.randint(1, 4)\n        image = np.flip(image, axis=axis).copy()\n        label = np.flip(label, axis=axis-1).copy()\n\n        return {'image': image, 'label': label}\n\n\ndef augment_gaussian_noise(data_sample, noise_variance=(0, 0.1)):\n    if noise_variance[0] == noise_variance[1]:\n        variance = noise_variance[0]\n    else:\n        variance = random.uniform(noise_variance[0], noise_variance[1])\n    data_sample = data_sample + np.random.normal(0.0, variance, size=data_sample.shape)\n    return data_sample\n\n\nclass GaussianNoise(object):\n    def __init__(self, noise_variance=(0, 0.1), p=0.5):\n        self.prob = p\n        self.noise_variance = noise_variance\n\n    def __call__(self, sample):\n        image = sample['image']\n        label = sample['label']\n        if np.random.uniform() < self.prob:\n            image = augment_gaussian_noise(image, self.noise_variance)\n        return {'image': image, 'label': label}\n\n\nclass ToTensor(object):\n    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n    def __call__(self, sample):\n        image = sample['image']\n        label = sample['label']\n\n        image = torch.from_numpy(image).float()\n        label = torch.from_numpy(label).long()\n\n        return {'image': image, 'label': label}\n\n\nclass BraTS(Dataset):\n    def __init__(self,data_path, file_path,transform=None):\n        with open(file_path, 'r') as f:\n            self.paths = [os.path.join(data_path, x.strip())+'_mri_norm2.h5' for x in f.readlines()]\n        self.transform = transform\n\n    def __getitem__(self, item):\n        h5f = h5py.File(self.paths[item], 'r')\n        image = h5f['image'][:]\n        label = h5f['label'][:]\n        #[0,1,2,4] -> [0,1,2,3]\n        label[label == 4] = 3\n        # print(image.shape)\n        sample = {'image': image, 'label': label}\n        if self.transform:\n            sample = self.transform(sample)\n        return sample['image'], sample['label']\n\n    def __len__(self):\n        return len(self.paths)\n\n    def collate(self, batch):\n        return [torch.cat(v) for v in zip(*batch)]","metadata":{"execution":{"iopub.status.busy":"2024-05-12T03:48:18.025777Z","iopub.execute_input":"2024-05-12T03:48:18.026209Z","iopub.status.idle":"2024-05-12T03:48:18.058726Z","shell.execute_reply.started":"2024-05-12T03:48:18.026176Z","shell.execute_reply":"2024-05-12T03:48:18.057527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loss and Evaluation Metrics","metadata":{}},{"cell_type":"code","source":"def Dice(output, target, eps=1e-3):\n    inter = torch.sum(output * target,dim=(1,2,-1)) + eps\n    union = torch.sum(output,dim=(1,2,-1)) + torch.sum(target,dim=(1,2,-1)) + eps * 2\n    x = 2 * inter / union\n    dice = torch.mean(x)\n    return dice\n\n\ndef cal_dice(output, target):\n    '''\n    output: (b, num_class, d, h, w)  target: (b, d, h, w)\n    dice1(ET):label4\n    dice2(TC):label1 + label4\n    dice3(WT): label1 + label2 + label4\n    Note: label 4 has been replaced with 3\n    '''\n    output = torch.argmax(output,dim=1)\n    dice1 = Dice((output == 3).float(), (target == 3).float())\n    dice2 = Dice(((output == 1) | (output == 3)).float(), ((target == 1) | (target == 3)).float())\n    dice3 = Dice((output != 0).float(), (target != 0).float())\n\n    return dice1, dice2, dice3\n\n\nclass Loss(nn.Module):\n    def __init__(self, n_classes, weight=None, alpha=0.5):\n        \"dice_loss_plus_cetr_weighted\"\n        super(Loss, self).__init__()\n        self.n_classes = n_classes\n        self.weight = weight.cuda()\n        # self.weight = weight\n        self.alpha = alpha\n\n    def forward(self, input, target):\n        # print(torch.unique(target))\n        smooth = 0.01\n\n        input1 = F.softmax(input, dim=1)\n        target1 = F.one_hot(target,self.n_classes)\n        input1 = rearrange(input1,'b n h w s -> b n (h w s)')\n        target1 = rearrange(target1,'b h w s n -> b n (h w s)')\n\n        input1 = input1[:, 1:, :]\n        target1 = target1[:, 1:, :].float()\n\n        # Calculate loss and dice loss on a batch basis, providing more stable training\n        inter = torch.sum(input1 * target1)\n        union = torch.sum(input1) + torch.sum(target1) + smooth\n        dice = 2.0 * inter / union\n\n        loss = F.cross_entropy(input,target, weight=self.weight)\n\n        total_loss = (1 - self.alpha) * loss + (1 - dice) * self.alpha\n\n        return total_loss","metadata":{"execution":{"iopub.status.busy":"2024-05-12T03:48:19.321743Z","iopub.execute_input":"2024-05-12T03:48:19.322154Z","iopub.status.idle":"2024-05-12T03:48:19.338491Z","shell.execute_reply.started":"2024-05-12T03:48:19.322121Z","shell.execute_reply":"2024-05-12T03:48:19.337025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"code","source":"# The learning rate is updated at each iteration, not just at each epoch.\n# Access the learning rate schedule at the current iteration index (scheduler[iter]).\n# Update the learning rate for the optimizer's parameter group.\n# help the model converge more smoothly and potentially achieve better performance.\ndef cosine_scheduler(base_value, final_value, epochs, niter_per_ep, warmup_epochs=0, start_warmup_value=0.):\n    warmup_schedule = np.array([])\n    warmup_iters = warmup_epochs * niter_per_ep\n    if warmup_epochs > 0:\n        warmup_schedule = np.linspace(start_warmup_value, base_value, warmup_iters)\n\n    iters = np.arange(epochs * niter_per_ep - warmup_iters)\n    schedule = final_value + 0.5 * (base_value - final_value) * (1 + np.cos(np.pi * iters / len(iters)))\n\n    schedule = np.concatenate((warmup_schedule, schedule))\n    assert len(schedule) == epochs * niter_per_ep\n    return schedule\n\n\ndef train_loop(model,optimizer,scheduler,criterion,train_loader,device,epoch):\n    model.train()\n    running_loss = 0\n    dice1_train = 0\n    dice2_train = 0\n    dice3_train = 0\n    pbar = tqdm(train_loader)\n    for it,(images,masks) in enumerate(pbar):\n        # update learning rate according to the schedule\n        it = len(train_loader) * epoch + it\n        param_group = optimizer.param_groups[0]\n        param_group['lr'] = scheduler[it]\n        # print(scheduler[it])\n\n        # [b,4,128,128,128] , [b,128,128,128]\n        images, masks = images.to(device),masks.to(device)\n        # [b,4,128,128,128], 4 segmentations\n        outputs = model(images)\n        # outputs = torch.softmax(outputs,dim=1)\n        loss = criterion(outputs, masks)\n        dice1, dice2, dice3 = cal_dice(outputs,masks)\n        pbar.desc = \"loss: {:.3f} \".format(loss.item())\n\n        running_loss += loss.item()\n        dice1_train += dice1.item()\n        dice2_train += dice2.item()\n        dice3_train += dice3.item()\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    loss = running_loss / len(train_loader)\n    dice1 = dice1_train / len(train_loader)\n    dice2 = dice2_train / len(train_loader)\n    dice3 = dice3_train / len(train_loader)\n    return {'loss':loss,'dice1':dice1,'dice2':dice2,'dice3':dice3}\n\n\ndef val_loop(model,criterion,val_loader,device):\n    model.eval()\n    running_loss = 0\n    dice1_val = 0\n    dice2_val = 0\n    dice3_val = 0\n    pbar = tqdm(val_loader)\n    with torch.no_grad():\n        for images, masks in pbar:\n            images, masks = images.to(device), masks.to(device)\n            outputs = model(images)\n            # outputs = torch.softmax(outputs,dim=1)\n\n            loss = criterion(outputs, masks)\n            dice1, dice2, dice3 = cal_dice(outputs, masks)\n\n            running_loss += loss.item()\n            dice1_val += dice1.item()\n            dice2_val += dice2.item()\n            dice3_val += dice3.item()\n            # pbar.desc = \"loss:{:.3f} dice1:{:.3f} dice2:{:.3f} dice3:{:.3f} \".format(loss,dice1,dice2,dice3)\n\n    loss = running_loss / len(val_loader)\n    dice1 = dice1_val / len(val_loader)\n    dice2 = dice2_val / len(val_loader)\n    dice3 = dice3_val / len(val_loader)\n    return {'loss':loss,'dice1':dice1,'dice2':dice2,'dice3':dice3}\n\n\ndef train(model,optimizer,scheduler,criterion,train_loader,\n          val_loader,epochs,device,train_log,valid_loss_min=999.0):\n    train_metrics_all = []\n    val_metrics_all = []\n    for e in range(epochs):\n        # train for epoch\n        train_metrics = train_loop(model,optimizer,scheduler,criterion,train_loader,device,e)\n        train_metrics_all.append(train_metrics)\n        # eval for epoch\n        val_metrics = val_loop(model,criterion,val_loader,device)\n        val_metrics_all.append(val_metrics)\n        info1 = \"Epoch:[{}/{}] valid_loss_min: {:.3f} train_loss: {:.3f} valid_loss: {:.3f} \".format(e+1,epochs,valid_loss_min,train_metrics[\"loss\"],val_metrics[\"loss\"])\n        info2 = \"Train--ET: {:.3f} TC: {:.3f} WT: {:.3f} \".format(train_metrics['dice1'],train_metrics['dice2'],train_metrics['dice3'])\n        info3 = \"Valid--ET: {:.3f} TC: {:.3f} WT: {:.3f} \".format(val_metrics['dice1'],val_metrics['dice2'],val_metrics['dice3'])\n        print(info1)\n        print(info2)\n        print(info3)\n        with open(train_log,'a') as f:\n            f.write(info1 + '\\n' + info2 + ' ' + info3 + '\\n')\n\n        if not os.path.exists(args.save_path):\n            os.makedirs(args.save_path)\n        save_file = {\"model\": model.state_dict(),\n                     \"optimizer\": optimizer.state_dict()}\n        if val_metrics['loss'] < valid_loss_min:\n            valid_loss_min = val_metrics['loss']\n            torch.save(save_file, args.weights)\n        else:\n            torch.save(save_file,os.path.join(args.save_path,'checkpoint{}.pth'.format(e+1)))\n    print(\"Finished Training!\")\n    return train_metrics_all, val_metrics_all\n\n\ndef main(args):\n    torch.manual_seed(args.seed)  # Set the seed for the CPU to ensure reproducible results\n    torch.cuda.manual_seed_all(args.seed)  # Set the seed for all GPUs to ensure reproducible results\n\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    # get datasets\n    patch_size = (160,160,128)\n    train_dataset = BraTS(args.data_path,args.train_txt,transform=transforms.Compose([\n        RandomRotFlip(), \n        RandomCrop(patch_size), \n        GaussianNoise(p=0.1), \n        ToTensor()\n    ]))\n    val_dataset = BraTS(args.data_path,args.valid_txt,transform=transforms.Compose([\n        CenterCrop(patch_size),\n        ToTensor()\n    ]))\n    test_dataset = BraTS(args.data_path,args.test_txt,transform=transforms.Compose([\n        CenterCrop(patch_size),\n        ToTensor()\n    ]))\n    # a glance at dataset\n    # d1 = test_dataset[0]\n    # image,label = d1\n    # print(image.shape)\n    # print(label.shape)\n    # print(np.unique(label))\n\n    # data loaders\n    train_loader = DataLoader(dataset=train_dataset, batch_size=args.batch_size, num_workers=12,   # num_worker=4\n                              shuffle=True, pin_memory=True)\n    val_loader = DataLoader(dataset=val_dataset, batch_size=args.batch_size, num_workers=12, shuffle=False,\n                            pin_memory=True)\n    test_loader = DataLoader(dataset=test_dataset, batch_size=args.batch_size, num_workers=12, shuffle=False,\n                             pin_memory=True)\n\n    print(\"using {} device.\".format(device))\n    print(\"using {} images for training, {} images for validation.\".format(len(train_dataset), len(val_dataset)))\n    # img,label = train_dataset[0]\n    # 1 - Necrotic tumor core (NT), 2 - Peritumoral edema (ED), 4 - Enhancing tumor (ET)\n    # Evaluation metrics: ET (label 4), TC (label 1 + label 4), WT (label 1 + label 2 + label 4)\n    if args.model == 'unet':\n        print(\"The model we are using  is U-Net.\")\n        model = UNet(in_channels=4,num_classes=4)\n    else:\n        print(\"The model we are using is Attention U-Net.\")\n        model = AttentionUNet(in_channels=4, num_classes=4)\n    # Use DataParallel for multi-GPU\n    if torch.cuda.device_count() > 1:\n        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n        model = nn.DataParallel(model)\n    model = model.to(device)\n    \n    num_params = 0\n    for param in model.parameters():\n        num_params += param.numel()\n    print(f'Total number of parameters: {num_params / 1e6:.3f} M')\n\n    criterion = Loss(n_classes=4, weight=torch.tensor([0.2, 0.3, 0.25, 0.25])).to(device)\n    optimizer = optim.SGD(model.parameters(),momentum=0.9, lr=0, weight_decay=5e-4)\n    scheduler = cosine_scheduler(base_value=args.lr,final_value=args.min_lr,epochs=args.epochs,\n                                 niter_per_ep=len(train_loader),warmup_epochs=args.warmup_epochs,start_warmup_value=5e-4)\n\n    # load training model\n    if os.path.exists(args.weights):\n        weight_dict = torch.load(args.weights, map_location=device)\n        model.load_state_dict(weight_dict['model'])\n        optimizer.load_state_dict(weight_dict['optimizer'])\n        print('Successfully loading checkpoint.')\n\n    train_metrics_all, val_metrics_all = train(model,optimizer,scheduler,criterion,train_loader,val_loader,args.epochs,device,train_log=args.train_log)\n\n    metrics1 = val_loop(model, criterion, train_loader, device)\n    metrics2 = val_loop(model, criterion, val_loader, device)\n    metrics3 = val_loop(model, criterion, test_loader, device)\n\n    # Finally, evaluate all the data again. \n    # Note that the model parameters used here are from the end of the training\n    print(\"Train -- loss: {:.3f} ET: {:.3f} TC: {:.3f} WT: {:.3f}\".format(metrics1['loss'], metrics1['dice1'],metrics1['dice2'], metrics1['dice3']))\n    print(\"Valid -- loss: {:.3f} ET: {:.3f} TC: {:.3f} WT: {:.3f}\".format(metrics2['loss'], metrics2['dice1'], metrics2['dice2'], metrics2['dice3']))\n    print(\"Test  -- loss: {:.3f} ET: {:.3f} TC: {:.3f} WT: {:.3f}\".format(metrics3['loss'], metrics3['dice1'], metrics3['dice2'], metrics3['dice3']))\n\n    return train_metrics_all, val_metrics_all","metadata":{"execution":{"iopub.status.busy":"2024-05-12T03:48:20.914556Z","iopub.execute_input":"2024-05-12T03:48:20.914928Z","iopub.status.idle":"2024-05-12T03:48:20.971930Z","shell.execute_reply.started":"2024-05-12T03:48:20.914887Z","shell.execute_reply":"2024-05-12T03:48:20.970596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    def __init__(self):\n        self.num_classes = 4\n        self.seed = 21\n        self.epochs = 60\n        self.warmup_epochs = 10\n        self.batch_size = 1\n        self.lr = 0.004\n        self.min_lr = 0.0015\n        self.data_path = train_set['out']\n        self.train_txt = './train.txt'\n        self.valid_txt = './valid.txt'\n        self.test_txt = './test.txt'\n        self.train_log = './results/UNet.txt'\n        self.weights = './results/UNet.pth'\n        self.save_path = './checkpoint/UNet'\n        self.model = 'aunet'  # unet or aunet\n\nargs = Config()","metadata":{"execution":{"iopub.status.busy":"2024-05-12T03:48:22.465576Z","iopub.execute_input":"2024-05-12T03:48:22.466386Z","iopub.status.idle":"2024-05-12T03:48:22.473641Z","shell.execute_reply.started":"2024-05-12T03:48:22.466348Z","shell.execute_reply":"2024-05-12T03:48:22.472500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if os.path.isfile(args.weights):\n    os.remove(args.weights)\nif os.path.isfile(self.train_log):\n    os.remove(self.train_log)\n\nimport gc\ngc.collect()\ntorch.cuda.empty_cache()\n\ntrain_metrics_all, val_metrics_all = main(args)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T22:59:23.755698Z","iopub.execute_input":"2024-05-11T22:59:23.756557Z","iopub.status.idle":"2024-05-12T02:41:08.538960Z","shell.execute_reply.started":"2024-05-11T22:59:23.756503Z","shell.execute_reply":"2024-05-12T02:41:08.536460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot","metadata":{}},{"cell_type":"code","source":"train_losses = [entry['loss'] for entry in train_metrics_all]\nvalid_losses = [entry['loss'] for entry in val_metrics_all]\n\nplt.figure(figsize=(10, 5))\nplt.plot(train_losses, label='Training Loss')\nplt.plot(valid_losses, label='Validation Loss')\n\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss Over Epochs')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-12T02:41:08.541651Z","iopub.execute_input":"2024-05-12T02:41:08.541983Z","iopub.status.idle":"2024-05-12T02:41:08.911800Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_et = [entry['dice1'] for entry in train_metrics_all]\nval_et = [entry['dice1'] for entry in val_metrics_all]\n\nplt.figure(figsize=(10, 5))\nplt.plot(train_et, label='Training ET')\nplt.plot(val_et, label='Validation ET')\n\nplt.xlabel('Epoch')\nplt.ylabel('ET Accracy')\nplt.title('Training and Validation ET Accuracy Over Epochs')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-12T02:41:09.189711Z","iopub.execute_input":"2024-05-12T02:41:09.189971Z","iopub.status.idle":"2024-05-12T02:41:09.552138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tc = [entry['dice2'] for entry in train_metrics_all]\nval_tc = [entry['dice2'] for entry in val_metrics_all]\n\nplt.figure(figsize=(10, 5))\nplt.plot(train_tc, label='Training TC')\nplt.plot(val_tc, label='Validation TC')\n\nplt.xlabel('Epoch')\nplt.ylabel('TC Accracy')\nplt.title('Training and Validation TC Accuracy Over Epochs')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-12T02:41:09.556741Z","iopub.execute_input":"2024-05-12T02:41:09.557073Z","iopub.status.idle":"2024-05-12T02:41:09.904685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_wt = [entry['dice3'] for entry in train_metrics_all]\nval_wt = [entry['dice3'] for entry in val_metrics_all]\n\nplt.figure(figsize=(10, 5))\nplt.plot(train_wt, label='Training WT')\nplt.plot(val_wt, label='Validation WT')\n\nplt.xlabel('Epoch')\nplt.ylabel('WT Accracy')\nplt.title('Training and Validation WT Accuracy Over Epochs')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-12T02:41:09.906041Z","iopub.execute_input":"2024-05-12T02:41:09.906347Z","iopub.status.idle":"2024-05-12T02:41:10.250437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_mean = [(entry['dice3']+entry['dice2']+entry['dice1'])/3 for entry in train_metrics_all]\nval_mean = [(entry['dice3']+entry['dice2']+entry['dice1'])/3 for entry in val_metrics_all]\n    \nplt.figure(figsize=(10, 5))\nplt.plot(train_mean, label='Training Average')\nplt.plot(val_mean, label='Validation Average')\n\nplt.xlabel('Epoch')\nplt.ylabel('Average Accracy')\nplt.title('Training and Validation Average Accuracy Over Epochs')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-12T02:41:10.251642Z","iopub.execute_input":"2024-05-12T02:41:10.251938Z","iopub.status.idle":"2024-05-12T02:41:10.607384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Testing by Sliding Window Inference","metadata":{}},{"cell_type":"code","source":"def calculate_metric_percase(pred, gt):\n    dice = metric.binary.dc(pred, gt)\n    jc = metric.binary.jc(pred, gt)\n    hd = metric.binary.hd95(pred, gt)\n    asd = metric.binary.asd(pred, gt)\n\n    return dice, jc, hd, asd\n\ndef test_single_case(net, image, stride_xy, stride_z, patch_size, num_classes=1):\n    print(image.shape)\n    c, ww, hh, dd = image.shape\n\n    sx = math.ceil((ww - patch_size[0]) / stride_xy) + 1\n    sy = math.ceil((hh - patch_size[1]) / stride_xy) + 1\n    sz = math.ceil((dd - patch_size[2]) / stride_z) + 1\n    # print(\"{}, {}, {}\".format(sx, sy, sz))\n    score_map = np.zeros((num_classes, ) + image.shape[1:]).astype(np.float32)\n    cnt = np.zeros(image.shape[1:]).astype(np.float32)\n\n    for x in range(0, sx):\n        xs = min(stride_xy*x, ww-patch_size[0])\n        for y in range(0, sy):\n            ys = min(stride_xy * y,hh-patch_size[1])\n            for z in range(0, sz):\n                zs = min(stride_z * z, dd-patch_size[2])\n                test_patch = image[:,xs:xs+patch_size[0], ys:ys+patch_size[1], zs:zs+patch_size[2]]\n                test_patch = np.expand_dims(test_patch,axis=0).astype(np.float32)\n                test_patch = torch.from_numpy(test_patch).cuda()\n                with torch.no_grad():\n                    y1 = net(test_patch)\n                    y = F.softmax(y1, dim=1)\n                y = y.cpu().data.numpy()\n                y = y[0,:,:,:,:]\n                score_map[:, xs:xs+patch_size[0], ys:ys+patch_size[1], zs:zs+patch_size[2]] \\\n                  = score_map[:, xs:xs+patch_size[0], ys:ys+patch_size[1], zs:zs+patch_size[2]] + y\n                cnt[xs:xs+patch_size[0], ys:ys+patch_size[1], zs:zs+patch_size[2]] \\\n                  = cnt[xs:xs+patch_size[0], ys:ys+patch_size[1], zs:zs+patch_size[2]] + 1\n    score_map = score_map/np.expand_dims(cnt,axis=0)\n    label_map = np.argmax(score_map, axis = 0)\n    return label_map, score_map\n\ndef test_all_case(net, image_list, num_classes=2, patch_size=(112, 112, 80), stride_xy=18, stride_z=4, save_result=True, test_save_path=None, preproc_fn=None):\n    total_metric = 0.0\n    for ith,image_path in enumerate(image_list):\n        h5f = h5py.File(image_path+'_mri_norm2.h5', 'r')\n        image = h5f['image'][:]\n        label = h5f['label'][:]\n        label[label==4] = 3  # Change label from 4 to 3\n        if preproc_fn is not None:\n            image = preproc_fn(image)\n        prediction, score_map = test_single_case(net, image, stride_xy, stride_z, patch_size, num_classes=num_classes)\n        print(np.unique(prediction),np.unique(label))\n\n        if np.sum(prediction)==0:\n            single_metric = (0,0,0,0)\n        else:\n            single_metric = calculate_metric_percase(prediction, label[:])\n        print('%02d,\\t%.5f, %.5f, %.5f, %.5f' % (ith, single_metric[0], single_metric[1], single_metric[2], single_metric[3]))\n        total_metric += np.asarray(single_metric)\n\n        if save_result:\n            nib.save(nib.Nifti1Image(prediction.astype(np.float32), np.eye(4)), test_save_path + \"%02d_pred.nii.gz\"%(ith))\n            # image only saves one modality\n            nib.save(nib.Nifti1Image(image[0].astype(np.float32), np.eye(4)), test_save_path + \"%02d_img.nii.gz\"%(ith))\n            nib.save(nib.Nifti1Image(label[:].astype(np.float32), np.eye(4)), test_save_path + \"%02d_gt.nii.gz\"%(ith))\n    avg_metric = total_metric / len(image_list)\n    print('average metric is {}'.format(avg_metric))\n\n    return avg_metric","metadata":{"execution":{"iopub.status.busy":"2024-05-12T03:48:29.915340Z","iopub.execute_input":"2024-05-12T03:48:29.915785Z","iopub.status.idle":"2024-05-12T03:48:29.943073Z","shell.execute_reply.started":"2024-05-12T03:48:29.915752Z","shell.execute_reply":"2024-05-12T03:48:29.941895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir predictions\n!mkdir predictions/unet","metadata":{"execution":{"iopub.status.busy":"2024-05-12T02:41:10.642488Z","iopub.execute_input":"2024-05-12T02:41:10.642830Z","iopub.status.idle":"2024-05-12T02:41:12.840985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# gc.collect()\n# torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-05-12T02:45:30.438675Z","iopub.execute_input":"2024-05-12T02:45:30.439049Z","iopub.status.idle":"2024-05-12T02:45:30.721609Z","shell.execute_reply.started":"2024-05-12T02:45:30.439018Z","shell.execute_reply":"2024-05-12T02:45:30.720402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_save_path = './predictions/unet/'\nsave_mode_path = args.weights\n\nif args.model == 'unet':\n    net = UNet(in_channels=4,num_classes=4)\nelse:\n    net = AttentionUNet(in_channels=4, num_classes=4)\ncounter = 0\nfor p in net.parameters():\n    counter += p.numel()\nprint('Param', counter)\nif torch.cuda.device_count() > 1:\n    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n    net = nn.DataParallel(net)\nnet = net.to(device)\n\nnet.load_state_dict(torch.load(save_mode_path)['model'])\nprint(\"init weight from {}\".format(save_mode_path))\nnet.eval()\n\nwith open(args.test_txt, 'r') as f:\n    image_list = [os.path.join(args.data_path, x.strip()) for x in f.readlines()]\nprint('Total number of images is', len(image_list))\n# print(image_list[0])\n\n# sliding_window_inference\navg_metric = test_all_case(net, image_list, num_classes=4,\n                            patch_size=(160,160,128), stride_xy=32, stride_z=16,\n                            save_result=True,test_save_path=test_save_path)   ","metadata":{"execution":{"iopub.status.busy":"2024-05-12T04:10:35.478981Z","iopub.execute_input":"2024-05-12T04:10:35.479994Z","iopub.status.idle":"2024-05-12T04:18:32.290164Z","shell.execute_reply.started":"2024-05-12T04:10:35.479957Z","shell.execute_reply":"2024-05-12T04:18:32.288849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pack up the output for viewing in the app like 'ITK-SNAP'\nshutil.make_archive(f'./{args.model}_lr{str(args.min_lr)[-3:]}', 'zip', test_save_path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Output Images Visualization","metadata":{}},{"cell_type":"code","source":"def display_nifti(file_path, title):\n    img = nib.load(file_path)\n    data = img.get_fdata()\n    \n    # Display the middle slice of each dimension\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    axes[0].imshow(data[data.shape[0] // 2, :, :], cmap='viridis')\n    axes[0].set_title(f'{title} - Axial')\n    axes[0].axis('off')\n    \n    axes[1].imshow(data[:, data.shape[1] // 2, :], cmap='viridis')\n    axes[1].set_title(f'{title} - Coronal')\n    axes[1].axis('off')\n    \n    axes[2].imshow(data[:, :, data.shape[2] // 2], cmap='viridis')\n    axes[2].set_title(f'{title} - Sagittal')\n    axes[2].axis('off')\n    plt.show()\n\n# Loop through all files and visualize them\nfor file_name in sorted(os.listdir(test_save_path)):\n    if file_name.endswith('.nii.gz'):\n        file_path = os.path.join(test_save_path, file_name)\n        title = file_name.split('.')[0]\n        display_nifti(file_path, title)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T04:19:21.669874Z","iopub.execute_input":"2024-05-12T04:19:21.670328Z","iopub.status.idle":"2024-05-12T04:19:40.592417Z","shell.execute_reply.started":"2024-05-12T04:19:21.670294Z","shell.execute_reply":"2024-05-12T04:19:40.591306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-05-12T04:18:32.291985Z","iopub.execute_input":"2024-05-12T04:18:32.292329Z","iopub.status.idle":"2024-05-12T04:18:34.381180Z","shell.execute_reply.started":"2024-05-12T04:18:32.292300Z","shell.execute_reply":"2024-05-12T04:18:34.379904Z"},"trusted":true},"execution_count":null,"outputs":[]}]}